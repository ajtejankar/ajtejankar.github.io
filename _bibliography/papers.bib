---
---


@InProceedings{cmsf,
    abbr      = {ECCV},
    author    = {KL Navaneet* and Ajinkya Tejankar* and Soroush Abbasi Koohpayegani* and Kossar Pourahmadi and Akshayvarun Subramanya and Hamed Pirsiavash},
    title     = {Constrained Mean Shift Using Distant Yet Related Neighbors for Representation Learning},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year      = {2022},
    abstract  = {We are interested in representation learning in self-supervised, supervised, and semi-supervised settings. Some recent self-supervised learning methods like mean-shift (MSF) cluster images by pulling the embedding of a query image to be closer to its nearest neighbors (NNs). Since most NNs are close to the query by design, the averaging may not affect the embedding of the query much. On the other hand, far away NNs may not be semantically related to the query. We generalize the mean-shift idea by constraining the search space of NNs using another source of knowledge so that NNs are far from the query while still being semantically related. We show that our method (1) outperforms MSF in SSL setting when the constraint utilizes a different augmentation of an image from the previous epoch, and (2) outperforms PAWS in semi-supervised setting with less training resources when the constraint ensures that the NNs have the same pseudo-label as the query.},
    selected  = {true},
}

@InProceedings{backdoor,
    abbr      = {CVPR},
    honor     = {Oral Presentation},
    award     = {Oral},
    author    = {Aniruddha Saha and Ajinkya Tejankar and Soroush Abbasi Koohpayegani and Ajinkya Pirsiavash},
    title     = {Backdoor Attacks on Self-supervised Learning},
    booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {13337-13346},
    abstract  = {Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich visual representations. State-of-the-art self-supervised methods for learning representations from images (e.g., MoCo, BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vulnerable to backdoor attacks -- where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practical in self-supervised learning, since the use of large unlabeled data makes data inspection to remove poisons prohibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a defense method based on knowledge distillation that succeeds in neutralizing the attack.},
    selected  = {true},
    pdf       = {https://arxiv.org/pdf/2105.10123.pdf},
    code      = {https://github.com/UMBCvision/SSL-Backdoor},
}

@article{bow,
    title   = {A fistful of words: Learning transferable visual models from bag-of-words supervision},
    author  = {Ajinkya Tejankar and Bichen Wu and Saining Xie and Madian Khabsa and Hamed Pirsiavash and Hamed Firooz},
    journal = {arXiv preprint arXiv:2112.13884},
    year    = {2021},
    abstract= {Using natural language as a supervision for training visual recognition models holds great promise. Recent works have shown that if such supervision is used in the form of alignment between images and captions in large training datasets, then the resulting aligned models perform well on zero-shot classification as downstream tasks2. In this paper, we focus on teasing out what parts of the language supervision are essential for training zero-shot image classification models. Through extensive and careful experiments, we show that: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for most of the image captions in the dataset. Surprisingly, we observe that this approach improves the zero-shot classification performance when combined with word balancing. 2) Using a BoW pretrained model, we can obtain more training data by generating pseudo-BoW captions on images that do not have a caption. Models trained on images with real and pseudo-BoW captions achieve stronger zero-shot performance. On ImageNet-1k zero-shot evaluation, our best model, that uses only 3M image-caption pairs, performs on-par with a CLIP model trained on 15M image-caption pairs (31.5% vs 31.3%).},
    selected= {true},
    pdf     = {https://arxiv.org/pdf/2112.13884.pdf},
}

@InProceedings{simreg,
    abbr      = {BMVC},
    author    = {KL Navaneet and Soroush Abbasi Koohpayegani and Ajinkya Tejankar and Hamed Pirsiavash},
    booktitle = {British Machine Vision Conference (BMVC)},
    title     = {SimReg: Regression as a Simple Yet Effective Tool for Self-supervised Knowledge Distillation},
    month     = {November},
    year      = {2021},
    abstract  = {We present a simple framework to improve performance of regression based knowledge distillation from self-supervised teacher networks. The teacher is trained using a standard self-supervised learning (SSL) technique. The student network is then trained to directly regress the teacher features (using MSE loss on normalized features). Importantly, the student architecture contains an additional multi-layer perceptron (MLP) head atop the CNN backbone during the distillation (training) stage. A deeper architecture provides the student higher capacity to predict the teacher representations. This additional MLP head can be removed during inference without hurting downstream performance. This is especially surprising since only the output of the MLP is trained to mimic the teacher and the backbone CNN features have a high MSE loss with the teacher features. This observation allows us to obtain better student models by using deeper models during distillation without altering the inference architecture. The train and test stage architectures are shown in the figure below.},
    selected  = {true},
    pdf       = {https://www.bmvc2021-virtualconference.com/assets/papers/1137.pdf},
    code      = {https://github.com/UCDvision/simreg},
}

@InProceedings{msf,
    abbr      = {ICCV},
    honor     = {Oral Presentation},
    award     = {Oral},
    author    = {Soroush Abbasi Koohpayegani* and Ajinkya Tejankar* and Hamed Pirsiavash},
    title     = {Mean Shift for Self-Supervised Learning},
    booktitle = {International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10326-10335},
    abstract  = {Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the images and then contrasting between the image clusters. We introduce a simple mean-shift al- gorithm that learns representations by grouping images to- gether without contrasting between them or adopting much of prior on the structure or number of the clusters. We simply “shift” the embedding of each image to be close to the “mean” of the neighbors of its augmentation. Since the closest neighbor is always another augmentation of the same image, our model will be identical to BYOL when using only one nearest neighbor instead of 5 used in our experiments. Our model achieves 72.4% on ImageNet lin- ear evaluation with ResNet50 at 200 epochs outperforming BYOL. Also, our method outperforms the SOTA by a large margin when using weak augmentations only, facilitating adoption of SSL for other modalities.},
    selected  = {true},
    pdf       = {https://arxiv.org/pdf/2105.07269.pdf},
    code      = {https://github.com/UMBCvision/MSF},
    website   = {https://umbcvision.github.io/MSF/},
}

@InProceedings{isd,
    abbr      = {ICCV},
    author    = {Ajinkya Tejankar* and Soroush Abbasi Koohpayegani*  and Vipin Pillai and Paolo Favaro and Hamed Pirsiavash},
    title     = {ISD: Self-Supervised Learning by Iterative Similarity Distillation},
    booktitle = {International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {9609-9618},
    abstract  = {Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to pull two augmentations of an image (positive pairs) closer com- pared to other random images (negative pairs). We argue that not all negative images are equally negative. Hence, we introduce a self-supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the stu- dent model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. Specifically, our method should handle unbal- anced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically simi- lar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negatives. Our method achieves comparable results to the state-of-the-art models.},
    selected  = {true},
    pdf       = {https://arxiv.org/pdf/2012.09259.pdf},
    code      = {https://github.com/UMBCvision/ISD},
    website   = {https://umbcvision.github.io/ISD/},
}

@InProceedings{compress,
    abbr      = {NeurIPS},
    author    = {Soroush Abbasi Koohpayegani* and Ajinkya Tejankar* and Hamed Pirsiavash},
    title     = {CompRess: Self-Supervised Learning by Compressing Representations},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    pages     = {12980-12992},
    month     = {December},
    year      = {2020},
    abstract  = {Self-supervised learning aims to learn good representations with unlabeled data. Recent works have shown that larger models benefit more from self-supervised learning than smaller models. As a result, the gap between supervised and self- supervised learning has been greatly reduced for larger models. In this work, instead of designing a new pseudo task for self-supervised learning, we develop a model compression method to compress an already learned, deep self-supervised model (teacher) to a smaller one (student). We train the student model so that it mimics the relative similarity between the datapoints in the teacher’s embed- ding space. For AlexNet, our method outperforms all previous methods including the fully supervised model on ImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor evaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the first time a self-supervised AlexNet has outper- formed supervised one on ImageNet classification.},
    selected  = {true},
    pdf       = {https://arxiv.org/pdf/2010.14713.pdf},
    code      = {https://github.com/UMBCvision/CompRess},
    website   = {https://umbcvision.github.io/CompRess/},
    
}
